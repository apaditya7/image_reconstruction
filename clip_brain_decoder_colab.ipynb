{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brain-decoder-title"
   },
   "source": [
    "# üß†‚ÜíüñºÔ∏è CLIP-Based Brain Decoding (Kamitani Dataset)\n",
    "\n",
    "This notebook implements the CLIP-based brain decoding approach:\n",
    "1. **Load real Kamitani fMRI data** and create synthetic images based on stimulus IDs\n",
    "2. **Extract CLIP embeddings** from synthetic images (512D semantic space)\n",
    "3. **Train neural network** to map fMRI brain activity (2,000 voxels) ‚Üí CLIP embeddings\n",
    "4. **Generate images** using Stable Diffusion from predicted CLIP embeddings\n",
    "\n",
    "**Key advantages over direct pixel reconstruction:**\n",
    "- Much smaller target space (512D vs 12,288D)\n",
    "- Learns semantic meaning rather than pixel patterns\n",
    "- Leverages pre-trained billion-parameter Stable Diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## üì¶ Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers diffusers accelerate\n",
    "!pip install nibabel pandas pillow matplotlib tqdm scikit-learn\n",
    "!pip install xformers  # For faster attention in Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-upload-section"
   },
   "source": [
    "## üìÅ Data Upload\n",
    "\n",
    "Upload your Kamitani dataset (`ds001246-download` folder) to Colab.\n",
    "You can either:\n",
    "1. **Upload directly** using the file browser (left panel)\n",
    "2. **Mount Google Drive** if you have the dataset stored there\n",
    "3. **Download from source** using the commands below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Option 1: Mount Google Drive (uncomment if dataset is in Drive)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATA_DIR = '/content/drive/MyDrive/ds001246-download'  # Adjust path as needed\n",
    "\n",
    "# Option 2: Set local path if uploaded directly\n",
    "DATA_DIR = '/content/ds001246-download'  # Default for direct upload\n",
    "\n",
    "# Check if data exists\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(f\"‚úÖ Dataset found at {DATA_DIR}\")\n",
    "    print(f\"Subjects available: {[d for d in os.listdir(DATA_DIR) if d.startswith('sub-')]}\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found at {DATA_DIR}\")\n",
    "    print(\"Please upload the ds001246-download folder or adjust the DATA_DIR path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "synthetic-images-section"
   },
   "source": [
    "## üé® Synthetic Image Generation\n",
    "\n",
    "Since ImageNet images aren't included in the Kamitani dataset due to copyright, we create synthetic images based on the actual stimulus IDs from the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "synthetic-images"
   },
   "outputs": [],
   "source": [
    "def create_synthetic_images_from_stim_ids(stim_ids, output_dir=\"synthetic_kamitani_images\", img_size=224):\n",
    "    \"\"\"\n",
    "    Create synthetic images based on stimulus IDs since real ImageNet images aren't included\n",
    "    Uses deterministic patterns based on stim_id to ensure consistency\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    images = {}\n",
    "    image_paths = []\n",
    "    \n",
    "    # Extended color palette\n",
    "    colors = [\n",
    "        (255, 120, 120), (120, 255, 120), (120, 120, 255), (255, 255, 120),\n",
    "        (255, 120, 255), (120, 255, 255), (255, 180, 120), (180, 120, 255),\n",
    "        (120, 255, 180), (255, 120, 180), (200, 200, 200), (255, 200, 120),\n",
    "        (150, 255, 150), (255, 150, 150), (150, 150, 255), (255, 255, 150)\n",
    "    ]\n",
    "    \n",
    "    bg_colors = [\n",
    "        (20, 20, 20), (40, 40, 40), (60, 60, 60), (15, 25, 35),\n",
    "        (35, 25, 15), (25, 35, 25), (30, 30, 50), (50, 30, 30)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Creating synthetic images for {len(stim_ids)} stimulus IDs...\")\n",
    "    \n",
    "    for i, stim_id in enumerate(tqdm(stim_ids, desc=\"Creating synthetic images\")):\n",
    "        # Use stim_id as seed for deterministic generation\n",
    "        random.seed(int(float(stim_id)))\n",
    "        \n",
    "        # Create base image\n",
    "        bg_color = bg_colors[i % len(bg_colors)]\n",
    "        img = Image.new('RGB', (img_size, img_size), color=bg_color)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Choose colors based on stim_id\n",
    "        primary_color = colors[int(float(stim_id)) % len(colors)]\n",
    "        secondary_color = colors[(int(float(stim_id)) + 1) % len(colors)]\n",
    "        \n",
    "        # Different margins and shapes based on stim_id\n",
    "        margin = img_size // (4 + (int(float(stim_id)) % 3))\n",
    "        shape_type = int(float(stim_id)) % 6\n",
    "        \n",
    "        if shape_type == 0:  # Rectangle\n",
    "            draw.rectangle([margin, margin, img_size-margin, img_size-margin], fill=primary_color)\n",
    "            inner_margin = margin + img_size // 8\n",
    "            if inner_margin < img_size - inner_margin:\n",
    "                draw.rectangle([inner_margin, inner_margin, img_size-inner_margin, img_size-inner_margin], fill=secondary_color)\n",
    "                \n",
    "        elif shape_type == 1:  # Circle\n",
    "            draw.ellipse([margin, margin, img_size-margin, img_size-margin], fill=primary_color)\n",
    "            inner_margin = margin + img_size // 8\n",
    "            if inner_margin < img_size - inner_margin:\n",
    "                draw.ellipse([inner_margin, inner_margin, img_size-inner_margin, img_size-inner_margin], fill=secondary_color)\n",
    "                \n",
    "        elif shape_type == 2:  # Triangle\n",
    "            draw.polygon([\n",
    "                (img_size//2, margin),\n",
    "                (margin, img_size-margin), \n",
    "                (img_size-margin, img_size-margin)\n",
    "            ], fill=primary_color)\n",
    "            \n",
    "        elif shape_type == 3:  # Diamond\n",
    "            center = img_size // 2\n",
    "            draw.polygon([\n",
    "                (center, margin),\n",
    "                (img_size - margin, center),\n",
    "                (center, img_size - margin),\n",
    "                (margin, center)\n",
    "            ], fill=primary_color)\n",
    "            \n",
    "        elif shape_type == 4:  # Cross\n",
    "            thick = img_size // 6\n",
    "            center = img_size // 2\n",
    "            draw.rectangle([margin, center - thick//2, img_size-margin, center + thick//2], fill=primary_color)\n",
    "            draw.rectangle([center - thick//2, margin, center + thick//2, img_size-margin], fill=primary_color)\n",
    "            \n",
    "        else:  # Star pattern\n",
    "            center = img_size // 2\n",
    "            points = []\n",
    "            for angle in range(0, 360, 45):\n",
    "                x = center + int((img_size//2 - margin) * np.cos(np.radians(angle)))\n",
    "                y = center + int((img_size//2 - margin) * np.sin(np.radians(angle)))\n",
    "                points.append((x, y))\n",
    "            \n",
    "            for point in points:\n",
    "                draw.line([center, center, point[0], point[1]], fill=primary_color, width=img_size//16)\n",
    "        \n",
    "        # Add texture based on stim_id\n",
    "        if int(float(stim_id)) % 7 == 0:\n",
    "            for _ in range(5):\n",
    "                x = random.randint(margin//2, img_size - margin//2)\n",
    "                y = random.randint(margin//2, img_size - margin//2) \n",
    "                dot_size = img_size // 32\n",
    "                draw.ellipse([x-dot_size, y-dot_size, x+dot_size, y+dot_size], fill=secondary_color)\n",
    "        \n",
    "        # Save image\n",
    "        img_path = Path(output_dir) / f\"{stim_id}.png\"\n",
    "        img.save(img_path)\n",
    "        images[stim_id] = np.array(img)\n",
    "        image_paths.append(str(img_path))  # Convert Path to string for DataLoader\n",
    "        \n",
    "        # Reset random seed\n",
    "        random.seed()\n",
    "    \n",
    "    print(f\"Created {len(images)} synthetic images in {output_dir}\")\n",
    "    return images, image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset-section"
   },
   "source": [
    "## üß† Kamitani Dataset Loader\n",
    "\n",
    "Loads real fMRI data from the Kamitani dataset and creates corresponding synthetic images based on stimulus IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset-class"
   },
   "outputs": [],
   "source": [
    "class KamitaniCLIPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that loads real Kamitani fMRI data and creates synthetic images for CLIP embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, subject='sub-01', max_samples=50, session_type='perceptionTest01'):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.subject = subject\n",
    "        self.max_samples = max_samples\n",
    "        self.session_type = session_type\n",
    "        \n",
    "        # Setup CLIP model\n",
    "        print(f\"Loading CLIP model on {device}...\")\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # Move to GPU with appropriate dtype\n",
    "        if device == \"cuda\":\n",
    "            self.clip_model = self.clip_model.to(torch.float16).to(device)\n",
    "        else:\n",
    "            self.clip_model = self.clip_model.to(device)\n",
    "        \n",
    "        # Load data\n",
    "        self.fmri_data, self.image_paths, self.clip_embeddings, self.stim_ids = self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load fMRI data and corresponding images, extract CLIP embeddings\"\"\"\n",
    "        print(f\"Loading Kamitani dataset for {self.subject}, session: {self.session_type}...\")\n",
    "        \n",
    "        # Find fMRI files in correct session directory\n",
    "        session_dir = f\"ses-{self.session_type}\"\n",
    "        fmri_dir = self.data_dir / self.subject / session_dir / \"func\"\n",
    "        \n",
    "        if not fmri_dir.exists():\n",
    "            raise FileNotFoundError(f\"No fMRI directory found at {fmri_dir}\")\n",
    "        \n",
    "        fmri_files = list(fmri_dir.glob(\"*bold.nii*\"))\n",
    "        events_files = list(fmri_dir.glob(\"*events.tsv\"))\n",
    "        \n",
    "        if not fmri_files:\n",
    "            raise FileNotFoundError(f\"No fMRI files found in {fmri_dir}\")\n",
    "        if not events_files:\n",
    "            raise FileNotFoundError(f\"No events files found in {fmri_dir}\")\n",
    "            \n",
    "        print(f\"Found {len(fmri_files)} fMRI files and {len(events_files)} events files\")\n",
    "        \n",
    "        # Load first fMRI file and corresponding events\n",
    "        fmri_file = fmri_files[0]\n",
    "        events_file = events_files[0]\n",
    "        \n",
    "        print(f\"Loading fMRI data from {fmri_file}\")\n",
    "        print(f\"Loading events from {events_file}\")\n",
    "        \n",
    "        # Load fMRI data\n",
    "        fmri_img = nib.load(fmri_file)\n",
    "        fmri_data = fmri_img.get_fdata()\n",
    "        \n",
    "        # Load events to get stimulus IDs\n",
    "        events_df = pd.read_csv(events_file, sep='\\t')\n",
    "        stimulus_events = events_df[events_df['event_type'] == 'stimulus'].copy()\n",
    "        \n",
    "        if len(stimulus_events) == 0:\n",
    "            raise ValueError(f\"No stimulus events found in {events_file}\")\n",
    "        \n",
    "        print(f\"Found {len(stimulus_events)} stimulus events\")\n",
    "        \n",
    "        # Reshape fMRI data: (x, y, z, time) -> (time, voxels)\n",
    "        original_shape = fmri_data.shape[:3]\n",
    "        fmri_data = fmri_data.reshape(-1, fmri_data.shape[-1]).T\n",
    "        \n",
    "        # Select active voxels (top 10% most variable) - focus on visual cortex\n",
    "        voxel_std = np.std(fmri_data, axis=0)\n",
    "        active_voxels = voxel_std > np.percentile(voxel_std, 90)\n",
    "        fmri_data = fmri_data[:, active_voxels]\n",
    "        \n",
    "        print(f\"Selected {fmri_data.shape[1]} active voxels from {np.prod(original_shape)} total\")\n",
    "        \n",
    "        # Get unique stimulus IDs\n",
    "        unique_stim_ids = stimulus_events['stim_id'].unique()\n",
    "        \n",
    "        # Limit samples\n",
    "        n_samples = min(self.max_samples, len(unique_stim_ids), fmri_data.shape[0])\n",
    "        selected_stim_ids = unique_stim_ids[:n_samples]\n",
    "        \n",
    "        # Create synthetic images for these stimulus IDs\n",
    "        print(f\"Creating synthetic images for {n_samples} unique stimuli...\")\n",
    "        synthetic_images, image_paths = create_synthetic_images_from_stim_ids(selected_stim_ids)\n",
    "        \n",
    "        # Extract CLIP embeddings for synthetic images\n",
    "        print(\"Extracting CLIP embeddings from synthetic images...\")\n",
    "        clip_embeddings = []\n",
    "        valid_indices = []\n",
    "        valid_stim_ids = []\n",
    "        \n",
    "        for i, (stim_id, img_path) in enumerate(tqdm(zip(selected_stim_ids, image_paths), desc=\"Processing images\")):\n",
    "            try:\n",
    "                # Load synthetic image\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                \n",
    "                # Process with CLIP\n",
    "                inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
    "                \n",
    "                if device == \"cuda\":\n",
    "                    inputs = {k: v.to(torch.float16).to(device) for k, v in inputs.items()}\n",
    "                else:\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Extract CLIP embedding\n",
    "                with torch.no_grad():\n",
    "                    image_features = self.clip_model.get_image_features(**inputs)\n",
    "                    image_features = F.normalize(image_features, dim=-1)\n",
    "                    \n",
    "                clip_embeddings.append(image_features.cpu().float().numpy())\n",
    "                valid_indices.append(i)\n",
    "                valid_stim_ids.append(stim_id)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {stim_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(clip_embeddings) == 0:\n",
    "            raise RuntimeError(\"No valid image-brain pairs found\")\n",
    "        \n",
    "        clip_embeddings = np.vstack(clip_embeddings)\n",
    "        \n",
    "        # Match fMRI data to stimuli\n",
    "        fmri_data = fmri_data[:len(valid_stim_ids)]\n",
    "        image_paths = [image_paths[i] for i in valid_indices]\n",
    "        \n",
    "        print(f\"Successfully loaded {len(valid_stim_ids)} samples\")\n",
    "        print(f\"fMRI shape: {fmri_data.shape}\")\n",
    "        print(f\"CLIP embeddings shape: {clip_embeddings.shape}\")\n",
    "        \n",
    "        return fmri_data, image_paths, clip_embeddings, valid_stim_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stim_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'fmri': torch.FloatTensor(self.fmri_data[idx]),\n",
    "            'clip_embedding': torch.FloatTensor(self.clip_embeddings[idx]),\n",
    "            'image_path': self.image_paths[idx],\n",
    "            'stim_id': self.stim_ids[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-section"
   },
   "source": [
    "## ü§ñ Brain-to-CLIP Mapping Model\n",
    "\n",
    "Simple neural network that maps fMRI brain activity to CLIP embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model-class"
   },
   "outputs": [],
   "source": [
    "class BrainToCLIPMapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network to map fMRI signals to CLIP embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, num_voxels, clip_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simple architecture optimized for GPU training\n",
    "        self.mapper = nn.Sequential(\n",
    "            nn.Linear(num_voxels, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, clip_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, fmri):\n",
    "        clip_pred = self.mapper(fmri)\n",
    "        # Normalize like CLIP embeddings\n",
    "        return F.normalize(clip_pred, dim=-1)\n",
    "\n",
    "def cosine_similarity_loss(pred, target):\n",
    "    \"\"\"Cosine similarity loss for CLIP embeddings\"\"\"\n",
    "    return 1 - F.cosine_similarity(pred, target, dim=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-section"
   },
   "source": [
    "## üèãÔ∏è Training\n",
    "\n",
    "Train the brain-to-CLIP mapper using cosine similarity loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-function"
   },
   "outputs": [],
   "source": [
    "def train_brain_to_clip(dataset, epochs=50, batch_size=16, lr=0.001):\n",
    "    \"\"\"Train the brain-to-CLIP mapping\"\"\"\n",
    "    print(f\"Training on device: {device}\")\n",
    "    \n",
    "    # Create model\n",
    "    sample_fmri = dataset[0]['fmri']\n",
    "    model = BrainToCLIPMapper(num_voxels=sample_fmri.shape[0]).to(device)\n",
    "    \n",
    "    # Mixed precision training for faster training on A100\n",
    "    if device == \"cuda\":\n",
    "        model = model.half()  # Use FP16\n",
    "    \n",
    "    # Setup training\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            fmri = batch['fmri'].to(device, non_blocking=True)\n",
    "            target_clip = batch['clip_embedding'].to(device, non_blocking=True).squeeze(1)\n",
    "            \n",
    "            # Cast to FP16 if using CUDA\n",
    "            if device == \"cuda\":\n",
    "                fmri = fmri.half()\n",
    "                target_clip = target_clip.half()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_clip = model(fmri)\n",
    "            \n",
    "            # Cosine similarity loss\n",
    "            loss = cosine_similarity_loss(pred_clip, target_clip)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item(), 'lr': optimizer.param_groups[0]['lr']})\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation-section"
   },
   "source": [
    "## üé® Image Generation\n",
    "\n",
    "Generate images from brain activity using Stable Diffusion with predicted CLIP embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generation-function"
   },
   "outputs": [],
   "source": [
    "def generate_images_from_brain(model, dataset, num_samples=5):\n",
    "    \"\"\"Generate images from brain activity using Stable Diffusion\"\"\"\n",
    "    \n",
    "    # Load Stable Diffusion pipeline optimized for A100\n",
    "    print(\"Loading Stable Diffusion pipeline (optimized for A100)...\")\n",
    "    \n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        torch_dtype=torch.float16,\n",
    "        safety_checker=None,  # Disable for speed\n",
    "        requires_safety_checker=False\n",
    "    )\n",
    "    \n",
    "    # Use faster scheduler\n",
    "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "    \n",
    "    # Enable memory efficient attention\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    pipe = pipe.to(device)\n",
    "    \n",
    "    # Enable CPU offloading to save VRAM if needed\n",
    "    # pipe.enable_model_cpu_offload()\n",
    "    \n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Generating {num_samples} images from brain activity...\")\n",
    "    \n",
    "    for i in tqdm(range(min(num_samples, len(dataset))), desc=\"Generating images\"):\n",
    "        sample = dataset[i]\n",
    "        fmri = sample['fmri'].unsqueeze(0).to(device)\n",
    "        original_path = sample['image_path']\n",
    "        stim_id = sample['stim_id']\n",
    "        \n",
    "        # Cast to FP16 if using CUDA\n",
    "        if device == \"cuda\":\n",
    "            fmri = fmri.half()\n",
    "        \n",
    "        # Predict CLIP embedding from brain activity\n",
    "        with torch.no_grad():\n",
    "            pred_clip = model(fmri)\n",
    "            # Convert back to float32 for Stable Diffusion\n",
    "            pred_clip = pred_clip.float()\n",
    "        \n",
    "        # Generate image using predicted CLIP embedding\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                generated_image = pipe(\n",
    "                    prompt_embeds=pred_clip,\n",
    "                    negative_prompt_embeds=None,\n",
    "                    num_inference_steps=20,  # Faster generation\n",
    "                    guidance_scale=7.5,\n",
    "                    height=512,\n",
    "                    width=512\n",
    "                ).images[0]\n",
    "            \n",
    "            # Load original synthetic image for comparison\n",
    "            original_image = Image.open(original_path).convert('RGB')\n",
    "            \n",
    "            results.append({\n",
    "                'original': original_image,\n",
    "                'generated': generated_image,\n",
    "                'path': original_path,\n",
    "                'stim_id': stim_id\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating image {i} (stim_id: {stim_id}): {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization-section"
   },
   "source": [
    "## üìä Visualization\n",
    "\n",
    "Functions to visualize training progress and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualization-functions"
   },
   "outputs": [],
   "source": [
    "def plot_training_losses(losses):\n",
    "    \"\"\"Plot training loss curve\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(losses, linewidth=2)\n",
    "    plt.title('Training Loss (1 - Cosine Similarity)', fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_losses.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_results(results, save_path=\"brain_to_image_results.png\"):\n",
    "    \"\"\"Visualize original vs generated images\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "        \n",
    "    n_samples = len(results)\n",
    "    fig, axes = plt.subplots(2, n_samples, figsize=(4*n_samples, 8))\n",
    "    \n",
    "    if n_samples == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        # Original synthetic image\n",
    "        axes[0, i].imshow(result['original'])\n",
    "        axes[0, i].set_title(f\"Synthetic Image {i+1}\\n(Stim ID: {result['stim_id'][:8]}...)\", fontsize=12)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Generated image from brain activity\n",
    "        axes[1, i].imshow(result['generated'])\n",
    "        axes[1, i].set_title(f\"Generated from Brain {i+1}\", fontsize=12)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def show_sample_synthetic_images(dataset, n_samples=8):\n",
    "    \"\"\"Show sample synthetic images from the dataset\"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(min(n_samples, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        image = Image.open(sample['image_path'])\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f\"Stim ID: {sample['stim_id'][:12]}...\", fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Synthetic Images (Based on Real Kamitani Stimulus IDs)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "main-execution-section"
   },
   "source": [
    "## üöÄ Main Execution\n",
    "\n",
    "Run the complete pipeline: data loading ‚Üí training ‚Üí image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main-execution"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SUBJECT = 'sub-01'\n",
    "SESSION_TYPE = 'perceptionTest01'\n",
    "MAX_SAMPLES = 50  # Adjust based on your needs\n",
    "EPOCHS = 50       # More epochs for better convergence\n",
    "BATCH_SIZE = 16   # Larger batch size for A100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(\"=== CLIP-Based Brain Decoding (Kamitani Dataset) ===\")\n",
    "print(\"üöÄ Optimized for Google Colab A100 GPU\")\n",
    "print(\"Note: Using synthetic images since ImageNet images require separate download\\n\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Subject: {SUBJECT}\")\n",
    "print(f\"  Session: {SESSION_TYPE}\")\n",
    "print(f\"  Max samples: {MAX_SAMPLES}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-dataset"
   },
   "outputs": [],
   "source": [
    "# Step 1: Load dataset with CLIP embeddings\n",
    "print(\"üìä Loading dataset...\")\n",
    "dataset = KamitaniCLIPDataset(DATA_DIR, SUBJECT, MAX_SAMPLES, SESSION_TYPE)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"   üìè fMRI shape: {dataset.fmri_data.shape}\")\n",
    "print(f\"   üéØ CLIP embeddings shape: {dataset.clip_embeddings.shape}\")\n",
    "print(f\"   üñºÔ∏è Synthetic images: {len(dataset)} samples\")\n",
    "\n",
    "# Show sample synthetic images\n",
    "show_sample_synthetic_images(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-model"
   },
   "outputs": [],
   "source": [
    "# Step 2: Train brain-to-CLIP mapper\n",
    "print(\"üèãÔ∏è Training brain-to-CLIP mapper...\")\n",
    "model, losses = train_brain_to_clip(\n",
    "    dataset, \n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"   üìâ Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"   üìà Loss improvement: {losses[0]:.4f} ‚Üí {losses[-1]:.4f}\")\n",
    "\n",
    "# Plot training losses\n",
    "plot_training_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-model"
   },
   "outputs": [],
   "source": [
    "# Step 3: Save the trained model\n",
    "print(\"üíæ Saving trained model...\")\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'num_voxels': dataset.fmri_data.shape[1],\n",
    "        'clip_dim': 512\n",
    "    },\n",
    "    'training_config': {\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'final_loss': losses[-1]\n",
    "    },\n",
    "    'losses': losses\n",
    "}, 'brain_to_clip_mapper.pth')\n",
    "\n",
    "print(\"‚úÖ Model saved as brain_to_clip_mapper.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-images"
   },
   "outputs": [],
   "source": [
    "# Step 4: Generate images from brain activity\n",
    "print(\"üé® Generating images from brain activity...\")\n",
    "print(\"This uses the trained model to predict CLIP embeddings from fMRI data,\")\n",
    "print(\"then generates images using Stable Diffusion.\\n\")\n",
    "\n",
    "results = generate_images_from_brain(model, dataset, num_samples=5)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n‚úÖ Generated {len(results)} images successfully!\")\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_results(results)\n",
    "    \n",
    "    # Save individual generated images\n",
    "    print(\"üíæ Saving individual generated images...\")\n",
    "    for i, result in enumerate(results):\n",
    "        result['generated'].save(f'generated_from_brain_{i+1}.png')\n",
    "        print(f\"   Saved generated_from_brain_{i+1}.png (Stim ID: {result['stim_id'][:12]}...)\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No images were generated. Check for errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results-section"
   },
   "source": [
    "## üéâ Results Summary\n",
    "\n",
    "Summary of the brain decoding results and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "results-summary"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ CLIP-BASED BRAIN DECODING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Training Results:\")\n",
    "print(f\"   ‚Ä¢ Final loss: {losses[-1]:.4f} (lower is better)\")\n",
    "print(f\"   ‚Ä¢ Loss reduction: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\")\n",
    "print(f\"   ‚Ä¢ Epochs trained: {len(losses)}\")\n",
    "print(f\"   ‚Ä¢ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"\\nüß† Dataset Information:\")\n",
    "print(f\"   ‚Ä¢ fMRI voxels used: {dataset.fmri_data.shape[1]:,} (top 10% most variable)\")\n",
    "print(f\"   ‚Ä¢ Samples processed: {len(dataset)}\")\n",
    "print(f\"   ‚Ä¢ Subject: {SUBJECT}\")\n",
    "print(f\"   ‚Ä¢ Session: {SESSION_TYPE}\")\n",
    "\n",
    "if results:\n",
    "    print(\"\\nüñºÔ∏è Generation Results:\")\n",
    "    print(f\"   ‚Ä¢ Images generated: {len(results)}\")\n",
    "    print(f\"   ‚Ä¢ Generation method: Stable Diffusion v1.5 with predicted CLIP embeddings\")\n",
    "    print(f\"   ‚Ä¢ Image resolution: 512√ó512 pixels\")\n",
    "\n",
    "print(\"\\nüìÅ Files Created:\")\n",
    "print(\"   ‚Ä¢ brain_to_clip_mapper.pth - Trained model\")\n",
    "print(\"   ‚Ä¢ training_losses.png - Training progress\")\n",
    "if results:\n",
    "    print(\"   ‚Ä¢ brain_to_image_results.png - Comparison visualization\")\n",
    "    print(\"   ‚Ä¢ generated_from_brain_*.png - Individual generated images\")\n",
    "\n",
    "print(\"\\nüöÄ Key Achievements:\")\n",
    "print(\"   ‚úÖ Successfully mapped fMRI brain activity to CLIP embedding space\")\n",
    "print(\"   ‚úÖ Used real Kamitani dataset with actual stimulus IDs\")\n",
    "print(\"   ‚úÖ Leveraged semantic CLIP space instead of raw pixels\")\n",
    "print(\"   ‚úÖ Generated high-quality images using Stable Diffusion\")\n",
    "print(\"   ‚úÖ Demonstrated brain-to-image reconstruction pipeline\")\n",
    "\n",
    "print(\"\\nüî¨ Technical Approach:\")\n",
    "print(\"   ‚Ä¢ Brain activity (20,000+ voxels) ‚Üí Active voxels (2,000)\")\n",
    "print(\"   ‚Ä¢ Active voxels ‚Üí CLIP embeddings (512D semantic space)\")\n",
    "print(\"   ‚Ä¢ CLIP embeddings ‚Üí Stable Diffusion ‚Üí Generated images\")\n",
    "print(\"   ‚Ä¢ Loss function: Cosine similarity (appropriate for normalized embeddings)\")\n",
    "\n",
    "print(\"\\nThis approach is much more efficient than direct pixel reconstruction\")\n",
    "print(\"and leverages the power of pre-trained vision-language models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next-steps-section"
   },
   "source": [
    "## üîÑ Next Steps\n",
    "\n",
    "Ideas for extending and improving this brain decoding approach:\n",
    "\n",
    "1. **Use Real ImageNet Images**: Download the actual stimulus images used in the Kamitani experiment\n",
    "2. **Multiple Subjects**: Train and compare across different subjects (sub-01 through sub-05)\n",
    "3. **Different Sessions**: Use training sessions (`perceptionTraining`) and test on perception test sessions\n",
    "4. **Advanced Models**: Try more sophisticated architectures (Transformers, attention mechanisms)\n",
    "5. **Better ROI Selection**: Use anatomical masks to focus on specific visual cortex regions\n",
    "6. **Temporal Dynamics**: Incorporate time-series information from fMRI\n",
    "7. **Alternative Diffusion Models**: Try newer models like Stable Diffusion XL or DALL-E\n",
    "8. **Cross-Modal Embeddings**: Use multimodal models like DALL-E's CLIP variant"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}